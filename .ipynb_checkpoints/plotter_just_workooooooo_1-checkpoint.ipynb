{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEbZN2wdzCfS",
        "outputId": "ea2e85a2-804c-440e-a330-89e8b928aec1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao1-9utqwjx9"
      },
      "source": [
        "## UNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muxrzBMnwjyB",
        "outputId": "cb128d77-8164-4ae8-9672-6aadd4a1f775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_c)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_c)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class encoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = conv_block(in_c, out_c)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv(inputs)\n",
        "        p = self.pool(x)\n",
        "\n",
        "        return x, p\n",
        "\n",
        "class decoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
        "        self.conv = conv_block(out_c+out_c, out_c)\n",
        "\n",
        "    def forward(self, inputs, skip):\n",
        "        x = self.up(inputs)\n",
        "        x = torch.cat([x, skip], axis=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class build_unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\" Encoder \"\"\"\n",
        "        self.e1 = encoder_block(3, 64)\n",
        "        self.e2 = encoder_block(64, 128)\n",
        "        self.e3 = encoder_block(128, 256)\n",
        "        self.e4 = encoder_block(256, 512)\n",
        "\n",
        "        \"\"\" Bottleneck \"\"\"\n",
        "        self.b = conv_block(512, 1024)\n",
        "\n",
        "        \"\"\" Decoder \"\"\"\n",
        "        self.d1 = decoder_block(1024, 512)\n",
        "        self.d2 = decoder_block(512, 256)\n",
        "        self.d3 = decoder_block(256, 128)\n",
        "        self.d4 = decoder_block(128, 64)\n",
        "\n",
        "        \"\"\" Classifier \"\"\"\n",
        "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" Encoder \"\"\"\n",
        "        s1, p1 = self.e1(inputs)\n",
        "        s2, p2 = self.e2(p1)\n",
        "        s3, p3 = self.e3(p2)\n",
        "        s4, p4 = self.e4(p3)\n",
        "\n",
        "        \"\"\" Bottleneck \"\"\"\n",
        "        b = self.b(p4)\n",
        "\n",
        "        \"\"\" Decoder \"\"\"\n",
        "        d1 = self.d1(b, s4)\n",
        "        d2 = self.d2(d1, s3)\n",
        "        d3 = self.d3(d2, s2)\n",
        "        d4 = self.d4(d3, s1)\n",
        "\n",
        "        outputs = self.outputs(d4)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn((2, 3, 512, 512))\n",
        "    f = build_unet()\n",
        "    y = f(x)\n",
        "    print(y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ42ZBwFwjyE"
      },
      "source": [
        "## LOSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6eNiDJjNwjyF"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "\n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "\n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "        Dice_BCE = BCE + dice_loss\n",
        "\n",
        "        return Dice_BCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nwgJh__wjyG"
      },
      "source": [
        "## UTILS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V7rRBbs5wjyG"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "\"\"\" Seeding the randomness. \"\"\"\n",
        "def seeding(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\"\"\" Create a directory. \"\"\"\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\"\"\" Calculate the time taken \"\"\"\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3unvGAyYwjyH"
      },
      "source": [
        "## DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sgVOhtI1wjyH"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DriveDataset(Dataset):\n",
        "    def __init__(self, images_path, masks_path):\n",
        "\n",
        "        self.images_path = images_path\n",
        "        self.masks_path = masks_path\n",
        "        self.n_samples = len(images_path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Reading image \"\"\"\n",
        "        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n",
        "        image = image/255.0 ## (512, 512, 3)\n",
        "        image = np.transpose(image, (2, 0, 1))  ## (3, 512, 512)\n",
        "        image = image.astype(np.float32)\n",
        "        image = torch.from_numpy(image)\n",
        "\n",
        "        \"\"\" Reading mask \"\"\"\n",
        "        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
        "        mask = mask/255.0   ## (512, 512)\n",
        "        mask = np.expand_dims(mask, axis=0) ## (1, 512, 512)\n",
        "        mask = mask.astype(np.float32)\n",
        "        mask = torch.from_numpy(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8gIDAIUwjyI"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DriveDataset(Dataset):\n",
        "    def __init__(self, images_path, masks_path):\n",
        "        self.images_path = images_path\n",
        "        self.masks_path = masks_path\n",
        "        self.n_samples = len(images_path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Reading image \"\"\"\n",
        "        image_path = self.images_path[index]\n",
        "        mask_path = self.masks_path[index]\n",
        "\n",
        "        # Debugging: Print paths\n",
        "        print(f\"Image path: {image_path}\")\n",
        "        print(f\"Mask path: {mask_path}\")\n",
        "\n",
        "        # Load image\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "        if image is None:\n",
        "            print(f\"Warning: Could not read image at {image_path}. Returning placeholder.\")\n",
        "            image = np.zeros((512, 512, 3), dtype=np.float32)  # Placeholder\n",
        "\n",
        "        image = image / 255.0  # Normalize to [0, 1]\n",
        "        image = np.transpose(image, (2, 0, 1))  # (3, 512, 512)\n",
        "        image = image.astype(np.float32)\n",
        "        image = torch.from_numpy(image)\n",
        "\n",
        "        # Load mask\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is None:\n",
        "            print(f\"Warning: Could not read mask at {mask_path}. Returning placeholder.\")\n",
        "            mask = np.zeros((512, 512), dtype=np.float32)  # Placeholder\n",
        "\n",
        "        mask = mask / 255.0  # Normalize to [0, 1]\n",
        "        mask = np.expand_dims(mask, axis=0)  # Add channel dimension (1, 512, 512)\n",
        "        mask = mask.astype(np.float32)\n",
        "        mask = torch.from_numpy(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n"
      ],
      "metadata": {
        "id": "m8pwudRd8VO0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train images: {train_x}\")\n",
        "print(f\"Train masks: {train_y}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTLaGIV2_wV_",
        "outputId": "81ea09ac-2e04-4ba0-d160-9531a007cf95"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: ['/content/drive/MyDrive/boundary_drive_file/split_data/train/images']\n",
            "Train masks: ['/content/drive/MyDrive/boundary_drive_file/split_data/train/masks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for path in train_x:\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Missing file: {path}\")\n"
      ],
      "metadata": {
        "id": "zB7Bvo1S_31E"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sE4eP3Hb_6gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HKNCZOFwjyI",
        "outputId": "7401d0ae-9df4-4446-8a4d-1e7471633993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Size: Train: 72, Valid: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p11_aug_31.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p11_mask_aug_31.png\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p11_aug_32.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p11_mask_aug_32.png\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p11_aug_34.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p11_mask_aug_34.png\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/lek_shore2_aug_8.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/lek_shore2_mask_aug_8.png\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p11_aug_30.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p11_mask_aug_30.png\n",
            "Batch input shape: torch.Size([2, 3, 512, 512])\n",
            "Batch target shape: torch.Size([2, 1, 512, 512])\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p15_aug_48.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p15_mask_aug_48.png\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p5_aug_72.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p5_mask_aug_72.png\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p18_aug_61.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p18_mask_aug_61.png\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p13_aug_37.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p13_mask_aug_37.png\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p17_aug_57.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p17_mask_aug_57.png\n",
            "Model output shape: torch.Size([2, 1, 512, 512])\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p7_aug_77.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p7_mask_aug_77.png\n",
            "Batch input shape: torch.Size([2, 3, 512, 512])\n",
            "Batch target shape: torch.Size([2, 1, 512, 512])\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p9_aug_87.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p9_mask_aug_87.png\n",
            "Model output shape: torch.Size([2, 1, 512, 512])\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p15_aug_49.png\n",
            "\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p15_mask_aug_49.pngBatch input shape: torch.Size([2, 3, 512, 512])\n",
            "Batch target shape: torch.Size([2, 1, 512, 512])\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/lek_shore1_aug_2.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/lek_shore1_mask_aug_2.png\n",
            "Model output shape: torch.Size([2, 1, 512, 512])\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p9_aug_88.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p9_mask_aug_88.png\n",
            "Batch input shape: torch.Size([2, 3, 512, 512])\n",
            "Batch target shape: torch.Size([2, 1, 512, 512])\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p11_aug_33.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p11_mask_aug_33.png\n",
            "Model output shape: torch.Size([2, 1, 512, 512])\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p17_aug_58.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p17_mask_aug_58.png\n",
            "Batch input shape: torch.Size([2, 3, 512, 512])\n",
            "Batch target shape: torch.Size([2, 1, 512, 512])\n",
            "Image path: /content/drive/MyDrive/boundary_drive_file/split_data/train/images/p10_aug_27.png\n",
            "Mask path: /content/drive/MyDrive/boundary_drive_file/split_data/train/masks/p10_mask_aug_27.png\n",
            "Model output shape: torch.Size([2, 1, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "# Train function\n",
        "def train(model, loader, optimizer, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for x, y in loader:\n",
        "        print(f\"Batch input shape: {x.shape}\")  # Debugging: Print batch input shape\n",
        "        print(f\"Batch target shape: {y.shape}\")  # Debugging: Print batch target shape\n",
        "\n",
        "        x = x.to(device, dtype=torch.float32)\n",
        "        y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        print(f\"Model output shape: {y_pred.shape}\")  # Debugging: Print model output shape\n",
        "\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    epoch_loss = epoch_loss / len(loader)\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "# Evaluate function\n",
        "def evaluate(model, loader, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "            y_pred = model(x)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    epoch_loss = epoch_loss / len(loader)\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    \"\"\" Directories \"\"\"\n",
        "    os.makedirs(\"files\", exist_ok=True)\n",
        "\n",
        "    \"\"\" Load dataset \"\"\"\n",
        "    train_x = sorted(glob(\"/content/drive/MyDrive/boundary_drive_file/split_data/train/images/*\"))\n",
        "    train_y = sorted(glob(\"/content/drive/MyDrive/boundary_drive_file/split_data/train/masks/*\"))\n",
        "    valid_x = sorted(glob(\"/content/drive/MyDrive/boundary_drive_file/split_data/test/images/*\"))\n",
        "    valid_y = sorted(glob(\"/content/drive/MyDrive/boundary_drive_file/split_data/test/masks/*\"))\n",
        "\n",
        "    assert len(train_x) == len(train_y), \"Mismatch between training images and masks count!\"\n",
        "    assert len(valid_x) == len(valid_y), \"Mismatch between validation images and masks count!\"\n",
        "\n",
        "    print(f\"Dataset Size: Train: {len(train_x)}, Valid: {len(valid_x)}\")\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    H, W = 512, 512\n",
        "    batch_size = 2\n",
        "    num_epochs = 50\n",
        "    lr = 1e-4\n",
        "    checkpoint_path = \"files/checkpoint.pth\"\n",
        "\n",
        "    \"\"\" Dataset and loader \"\"\"\n",
        "    train_dataset = DriveDataset(train_x, train_y)\n",
        "    valid_dataset = DriveDataset(valid_x, valid_y)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    valid_loader = DataLoader(\n",
        "        dataset=valid_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    \"\"\" Device and model \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = build_unet()  # Replace with your U-Net model\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
        "    loss_fn = DiceBCELoss()  # Replace with your custom loss function\n",
        "\n",
        "    \"\"\" Training loop \"\"\"\n",
        "    best_valid_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
        "        valid_loss = evaluate(model, valid_loader, loss_fn, device)\n",
        "\n",
        "        \"\"\" Save the best model \"\"\"\n",
        "        if valid_loss < best_valid_loss:\n",
        "            print(f\"Validation loss improved from {best_valid_loss:.4f} to {valid_loss:.4f}. Saving model...\")\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "        scheduler.step(valid_loss)\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = divmod(int(end_time - start_time), 60)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Time: {epoch_mins}m {epoch_secs}s\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/boundary_drive_file/split_data/test/images\n",
        "# !ls /content/drive/MyDrive/boundary_drive_file/split_data/test/masks\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh5tmEHK_117",
        "outputId": "36713f16-c28a-41dc-8080-2651d295501f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lek_shore1_aug_0.png\t     lek_shore5__2014_aug_22.png  p14_aug_40.png  p5_aug_70.png\n",
            "lek_shore1_aug_4.png\t     p10_aug_26.png\t\t  p14_aug_44.png  p8_aug_81.png\n",
            "lek_shore3__2018_aug_10.png  p10_aug_28.png\t\t  p17_aug_55.png  p9_aug_86.png\n",
            "lek_shore3__2018_aug_12.png  p13_aug_35.png\t\t  p17_aug_56.png\n",
            "lek_shore4__2016_aug_18.png  p13_aug_39.png\t\t  p4_aug_65.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ASpubKEN_zxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sWZvBAaT_tqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ftnsqW65cyY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}